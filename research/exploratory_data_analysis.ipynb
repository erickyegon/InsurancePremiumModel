{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Exploratory Data Analysis for Insurance Premium Prediction\n",
                "\n",
                "This notebook performs a comprehensive exploratory data analysis on the insurance premium dataset to understand the data characteristics, identify patterns, and inform feature engineering and modeling decisions.\n",
                "\n",
                "**Author:** Erick K. Yegon, PhD (keyegon@gmail.com)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "source": [
                "# Import necessary libraries\n",
                "import os\n",
                "import sys\n",
                "import numpy as np\n",
                "import pandas as pd\n",
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns\n",
                "from scipy import stats\n",
                "import warnings\n",
                "from statsmodels.graphics.gofplots import qqplot\n",
                "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
                "from sklearn.preprocessing import StandardScaler\n",
                "from sklearn.decomposition import PCA\n",
                "from sklearn.feature_selection import mutual_info_regression\n",
                "\n",
                "# Set plot style\n",
                "plt.style.use('seaborn-whitegrid')\n",
                "sns.set_palette('viridis')\n",
                "warnings.filterwarnings('ignore')\n",
                "\n",
                "# Add the parent directory to the path so we can import the package\n",
                "sys.path.append(os.path.abspath('..'))"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Data Loading and Initial Inspection"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "source": [
                "# Load the data\n",
                "data_path = '../data/premiums.xlsx'\n",
                "df = pd.read_excel(data_path)\n",
                "\n",
                "# Rename columns with spaces to use underscores for consistency\n",
                "if 'Number Of Dependants' in df.columns:\n",
                "    df = df.rename(columns={'Number Of Dependants': 'Number_Of_Dependants'})\n",
                "\n",
                "if 'Medical History' in df.columns:\n",
                "    df = df.rename(columns={'Medical History': 'Medical_History'})\n",
                "\n",
                "# Display basic information\n",
                "print(f\"Dataset shape: {df.shape}\")\n",
                "print(f\"Number of samples: {df.shape[0]}\")\n",
                "print(f\"Number of features: {df.shape[1]}\")\n",
                "print(f\"\\nColumns: {df.columns.tolist()}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "source": [
                "# Display the first few rows of the dataset\n",
                "df.head()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "source": [
                "# Get data types and summary statistics\n",
                "print(\"Data Types:\")\n",
                "print(df.dtypes)\n",
                "\n",
                "print(\"\\nSummary Statistics:\")\n",
                "df.describe(include='all').T"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Data Quality Assessment"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "source": [
                "# Check for missing values\n",
                "missing_values = df.isnull().sum()\n",
                "missing_percentage = (missing_values / len(df)) * 100\n",
                "\n",
                "missing_df = pd.DataFrame({\n",
                "    'Missing Values': missing_values,\n",
                "    'Percentage': missing_percentage\n",
                "})\n",
                "\n",
                "print(\"Missing Values Analysis:\")\n",
                "print(missing_df[missing_df['Missing Values'] > 0])\n",
                "\n",
                "# Visualize missing values if any\n",
                "if missing_values.sum() > 0:\n",
                "    plt.figure(figsize=(12, 6))\n",
                "    plt.bar(missing_df.index, missing_df['Percentage'])\n",
                "    plt.title('Percentage of Missing Values by Feature')\n",
                "    plt.xlabel('Features')\n",
                "    plt.ylabel('Percentage Missing')\n",
                "    plt.xticks(rotation=90)\n",
                "    plt.tight_layout()\n",
                "    plt.show()\n",
                "else:\n",
                "    print(\"No missing values found in the dataset.\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "source": [
                "# Check for duplicates\n",
                "duplicates = df.duplicated().sum()\n",
                "print(f\"Number of duplicate rows: {duplicates}\")\n",
                "if duplicates > 0:\n",
                "    print(\"Sample of duplicate rows:\")\n",
                "    df[df.duplicated(keep='first')].head()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "source": [
                "# Check for outliers in numerical features\n",
                "numerical_features = df.select_dtypes(include=['int64', 'float64']).columns\n",
                "\n",
                "# Function to detect outliers using IQR method\n",
                "def detect_outliers_iqr(df, column):\n",
                "    Q1 = df[column].quantile(0.25)\n",
                "    Q3 = df[column].quantile(0.75)\n",
                "    IQR = Q3 - Q1\n",
                "    lower_bound = Q1 - 1.5 * IQR\n",
                "    upper_bound = Q3 + 1.5 * IQR\n",
                "    outliers = df[(df[column] < lower_bound) | (df[column] > upper_bound)]\n",
                "    return outliers, lower_bound, upper_bound, len(outliers)\n",
                "\n",
                "# Function to detect outliers using Z-score method\n",
                "def detect_outliers_zscore(df, column, threshold=3):\n",
                "    z_scores = np.abs(stats.zscore(df[column]))\n",
                "    outliers = df[z_scores > threshold]\n",
                "    return outliers, len(outliers)\n",
                "\n",
                "# Display outlier information for numerical features\n",
                "print(\"Outlier Analysis:\")\n",
                "for column in numerical_features:\n",
                "    if column != 'Annual_Premium_Amount':  # Skip the target variable\n",
                "        _, lower, upper, count_iqr = detect_outliers_iqr(df, column)\n",
                "        _, count_zscore = detect_outliers_zscore(df, column)\n",
                "        print(f\"\\n{column}:\")\n",
                "        print(f\"  IQR Method: {count_iqr} outliers (bounds: {lower:.2f}, {upper:.2f})\")\n",
                "        print(f\"  Z-score Method: {count_zscore} outliers (threshold: 3)\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "source": [
                "# Visualize outliers using box plots\n",
                "plt.figure(figsize=(15, 10))\n",
                "for i, column in enumerate(numerical_features):\n",
                "    if column != 'Annual_Premium_Amount':  # Skip the target variable for now\n",
                "        plt.subplot(2, 3, i+1)\n",
                "        sns.boxplot(y=df[column])\n",
                "        plt.title(f'Boxplot of {column}')\n",
                "plt.tight_layout()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Distribution Analysis"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "source": [
                "# Analyze distributions of numerical features\n",
                "plt.figure(figsize=(15, 12))\n",
                "for i, column in enumerate(numerical_features):\n",
                "    plt.subplot(3, 3, i+1)\n",
                "    sns.histplot(df[column], kde=True)\n",
                "    plt.title(f'Distribution of {column}')\n",
                "    \n",
                "    # Add normality test results\n",
                "    if len(df[column]) > 3:  # Need at least 3 samples for normality test\n",
                "        stat, p = stats.shapiro(df[column].sample(min(5000, len(df[column]))))\n",
                "        plt.annotate(f'Shapiro-Wilk: p={p:.4f}', xy=(0.05, 0.95), xycoords='axes fraction')\n",
                "plt.tight_layout()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "source": [
                "# QQ plots for numerical features to check normality\n",
                "plt.figure(figsize=(15, 12))\n",
                "for i, column in enumerate(numerical_features):\n",
                "    plt.subplot(3, 3, i+1)\n",
                "    qqplot(df[column], line='s', ax=plt.gca())\n",
                "    plt.title(f'QQ Plot of {column}')\n",
                "plt.tight_layout()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "source": [
                "# Analyze distributions of categorical features\n",
                "categorical_features = df.select_dtypes(include=['object']).columns\n",
                "\n",
                "for column in categorical_features:\n",
                "    plt.figure(figsize=(12, 6))\n",
                "    value_counts = df[column].value_counts()\n",
                "    \n",
                "    # Bar plot\n",
                "    plt.subplot(1, 2, 1)\n",
                "    sns.countplot(y=column, data=df, order=value_counts.index)\n",
                "    plt.title(f'Count of {column}')\n",
                "    plt.xlabel('Count')\n",
                "    \n",
                "    # Pie chart\n",
                "    plt.subplot(1, 2, 2)\n",
                "    plt.pie(value_counts, labels=value_counts.index, autopct='%1.1f%%')\n",
                "    plt.title(f'Percentage of {column}')\n",
                "    \n",
                "    plt.tight_layout()\n",
                "    plt.show()\n",
                "    \n",
                "    # Print category counts and percentages\n",
                "    print(f\"\\n{column} - Category Distribution:\")\n",
                "    category_df = pd.DataFrame({\n",
                "        'Count': value_counts,\n",
                "        'Percentage': (value_counts / len(df)) * 100\n",
                "    })\n",
                "    print(category_df)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Target Variable Analysis"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "source": [
                "# Analyze the target variable (Annual_Premium_Amount)\n",
                "target = 'Annual_Premium_Amount'\n",
                "\n",
                "plt.figure(figsize=(15, 5))\n",
                "\n",
                "# Histogram\n",
                "plt.subplot(1, 3, 1)\n",
                "sns.histplot(df[target], kde=True)\n",
                "plt.title(f'Distribution of {target}')\n",
                "\n",
                "# Box plot\n",
                "plt.subplot(1, 3, 2)\n",
                "sns.boxplot(y=df[target])\n",
                "plt.title(f'Boxplot of {target}')\n",
                "\n",
                "# Log transformation to check if it normalizes the distribution\n",
                "plt.subplot(1, 3, 3)\n",
                "sns.histplot(np.log1p(df[target]), kde=True)\n",
                "plt.title(f'Log Distribution of {target}')\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.show()\n",
                "\n",
                "# Summary statistics for the target variable\n",
                "print(f\"\\n{target} - Summary Statistics:\")\n",
                "target_stats = df[target].describe()\n",
                "print(target_stats)\n",
                "\n",
                "# Additional statistics\n",
                "print(f\"\\nSkewness: {df[target].skew():.4f}\")\n",
                "print(f\"Kurtosis: {df[target].kurtosis():.4f}\")\n",
                "\n",
                "# Normality test\n",
                "stat, p = stats.shapiro(df[target].sample(min(5000, len(df[target]))))\n",
                "print(f\"Shapiro-Wilk Test: statistic={stat:.4f}, p-value={p:.4f}\")\n",
                "if p < 0.05:\n",
                "    print(\"The target variable is not normally distributed.\")\n",
                "else:\n",
                "    print(\"The target variable appears to be normally distributed.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. Relationship Analysis"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "source": [
                "# Correlation analysis for numerical features\n",
                "correlation_matrix = df[numerical_features].corr()\n",
                "\n",
                "plt.figure(figsize=(12, 10))\n",
                "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt='.2f', linewidths=0.5)\n",
                "plt.title('Correlation Matrix of Numerical Features')\n",
                "plt.tight_layout()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "source": [
                "# Pairplot for numerical features\n",
                "numerical_sample = df[numerical_features].sample(min(1000, len(df)))\n",
                "sns.pairplot(numerical_sample)\n",
                "plt.suptitle('Pairplot of Numerical Features', y=1.02)\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "source": [
                "# Relationship between categorical features and target\n",
                "for column in categorical_features:\n",
                "    plt.figure(figsize=(12, 6))\n",
                "    sns.boxplot(x=column, y=target, data=df)\n",
                "    plt.title(f'Relationship between {column} and {target}')\n",
                "    plt.xticks(rotation=90)\n",
                "    plt.tight_layout()\n",
                "    plt.show()\n",
                "    \n",
                "    # ANOVA test to check if the means are significantly different\n",
                "    groups = [df[df[column] == category][target] for category in df[column].unique()]\n",
                "    f_stat, p_value = stats.f_oneway(*groups)\n",
                "    print(f\"ANOVA test for {column}: F-statistic={f_stat:.4f}, p-value={p_value:.4f}\")\n",
                "    if p_value < 0.05:\n",
                "        print(f\"The mean {target} is significantly different across {column} categories.\\n\")\n",
                "    else:\n",
                "        print(f\"No significant difference in mean {target} across {column} categories.\\n\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "source": [
                "# Relationship between numerical features and target\n",
                "for column in numerical_features:\n",
                "    if column != target:\n",
                "        plt.figure(figsize=(10, 6))\n",
                "        sns.scatterplot(x=column, y=target, data=df, alpha=0.5)\n",
                "        plt.title(f'Relationship between {column} and {target}')\n",
                "        \n",
                "        # Add regression line\n",
                "        sns.regplot(x=column, y=target, data=df, scatter=False, color='red')\n",
                "        \n",
                "        # Calculate correlation\n",
                "        corr, p = stats.pearsonr(df[column], df[target])\n",
                "        plt.annotate(f'Pearson r: {corr:.4f} (p={p:.4f})', xy=(0.05, 0.95), xycoords='axes fraction')\n",
                "        \n",
                "        plt.tight_layout()\n",
                "        plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 6. Feature Importance Analysis"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "source": [
                "# Calculate mutual information for numerical features\n",
                "X_numeric = df[numerical_features].drop(columns=[target])\n",
                "y = df[target]\n",
                "\n",
                "# Calculate mutual information\n",
                "mi_scores = mutual_info_regression(X_numeric, y)\n",
                "mi_df = pd.DataFrame({'Feature': X_numeric.columns, 'Mutual Information': mi_scores})\n",
                "mi_df = mi_df.sort_values('Mutual Information', ascending=False)\n",
                "\n",
                "plt.figure(figsize=(10, 6))\n",
                "sns.barplot(x='Mutual Information', y='Feature', data=mi_df)\n",
                "plt.title('Feature Importance (Mutual Information)')\n",
                "plt.tight_layout()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "source": [
                "# One-hot encode categorical features for further analysis\n",
                "df_encoded = pd.get_dummies(df, columns=categorical_features, drop_first=True)\n",
                "\n",
                "# Calculate VIF for numerical features to check multicollinearity\n",
                "X_numeric = df[numerical_features].drop(columns=[target])\n",
                "vif_data = pd.DataFrame()\n",
                "vif_data['Feature'] = X_numeric.columns\n",
                "vif_data['VIF'] = [variance_inflation_factor(X_numeric.values, i) for i in range(X_numeric.shape[1])]\n",
                "\n",
                "print(\"Variance Inflation Factor (VIF) for Numerical Features:\")\n",
                "print(vif_data.sort_values('VIF', ascending=False))"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 7. Dimensionality Reduction and Visualization"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "source": [
                "# PCA for visualization\n",
                "# Prepare data for PCA (standardize and include only numerical features)\n",
                "X_for_pca = df[numerical_features].drop(columns=[target])\n",
                "X_scaled = StandardScaler().fit_transform(X_for_pca)\n",
                "\n",
                "# Apply PCA\n",
                "pca = PCA(n_components=2)\n",
                "principal_components = pca.fit_transform(X_scaled)\n",
                "pca_df = pd.DataFrame(data=principal_components, columns=['PC1', 'PC2'])\n",
                "\n",
                "# Add target variable for coloring\n",
                "pca_df['Annual_Premium_Amount'] = df[target]\n",
                "\n",
                "# Visualize PCA results\n",
                "plt.figure(figsize=(10, 8))\n",
                "sns.scatterplot(x='PC1', y='PC2', hue='Annual_Premium_Amount', data=pca_df, palette='viridis', alpha=0.7)\n",
                "plt.title('PCA of Numerical Features')\n",
                "plt.xlabel(f'Principal Component 1 ({pca.explained_variance_ratio_[0]:.2%} variance)')\n",
                "plt.ylabel(f'Principal Component 2 ({pca.explained_variance_ratio_[1]:.2%} variance)')\n",
                "plt.colorbar(label='Annual Premium Amount')\n",
                "plt.tight_layout()\n",
                "plt.show()\n",
                "\n",
                "# Print explained variance ratio\n",
                "print(f\"Explained variance ratio: {pca.explained_variance_ratio_}\")\n",
                "print(f\"Total explained variance: {sum(pca.explained_variance_ratio_):.2%}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 8. Key Insights and Recommendations"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Key Insights from EDA:\n",
                "\n",
                "1. **Data Quality**:\n",
                "   - The dataset contains [X] samples and [Y] features\n",
                "   - Missing values: [summary of missing values findings]\n",
                "   - Outliers: [summary of outlier analysis]\n",
                "\n",
                "2. **Feature Distributions**:\n",
                "   - Numerical features: [summary of distribution analysis]\n",
                "   - Categorical features: [summary of category distributions]\n",
                "   - Target variable: [summary of target distribution]\n",
                "\n",
                "3. **Relationships**:\n",
                "   - Strong correlations between [feature pairs]\n",
                "   - Significant relationships between [categorical features] and the target\n",
                "   - Most important features based on mutual information: [top features]\n",
                "\n",
                "4. **Potential Issues**:\n",
                "   - Multicollinearity between [features]\n",
                "   - Skewed distributions in [features]\n",
                "   - Class imbalance in [categorical features]\n",
                "\n",
                "### Recommendations for Data Preprocessing:\n",
                "\n",
                "1. **Handling Missing Values**:\n",
                "   - [Specific recommendations based on findings]\n",
                "\n",
                "2. **Outlier Treatment**:\n",
                "   - [Specific recommendations based on findings]\n",
                "\n",
                "3. **Feature Transformations**:\n",
                "   - Apply log transformation to the target variable to normalize its distribution\n",
                "   - Apply appropriate transformations to skewed numerical features\n",
                "\n",
                "4. **Feature Engineering Opportunities**:\n",
                "   - Create interaction terms between [features]\n",
                "   - Develop composite risk scores based on [features]\n",
                "   - Bin continuous variables like Age into meaningful categories\n",
                "\n",
                "5. **Feature Selection**:\n",
                "   - Consider removing highly correlated features\n",
                "   - Focus on features with high mutual information scores\n",
                "\n",
                "These insights and recommendations will guide our feature engineering and modeling approaches in the subsequent notebooks."
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.10"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}